https://en.wikipedia.org/wiki/Artificial_neural_network

ANNs are computing systems vaguely inspired by the biological neural networks of animal brains.
Such systems "learn" (i.e. progressively improve performance on) tasks by considering examples, 
generally without task-specific programming.

An ANN is based on a collection of connected units or nodes called artificial neurons 
(a simplified version of biological neurons in an animal brain). Each connection (a simplified 
version of a synapse) between artificial neurons can transmit a signal from one to another.

In common ANN implementations, the signal at a connection between artificial neurons is a real 
number, and the output of each artificial neuron is calculated by a non-linear function of the 
sum of its inputs. Artificial neurons and connections typically have a weight that adjusts as 
learning proceeds.

The original goal of the ANN approach was to solve problems in the same way that a human brain 
would. However, over time, attention focused on matching specific tasks, leading to deviations 
from biology.

## HISTORY

Warren McCulloch and Walter Pitts (1943) created a computational model for neural networks based 
on mathematics and algorithms called threshold logic. This model paved the way for neural network 
research to split into two approaches. One approach focused on biological processes in the brain 
while the other focused on the application of neural networks to artificial intelligence. This work 
led to work on nerve networks and their link to finite automata.

"A Logical Calculus of Ideas Immanent in Nervous Activity" (1943) proposed the first mathematical
model of a neural network.

### Hebbian learning

In the late 1940s, D.O. Hebb created a learning hypothesis based on the mechanism of neural 
plasticity that became known as Hebbian learning. Hebbian learning is unsupervised learning. 
Researchers started applying these ideas to computational models in 1948 with Turing's B-type 
machines.

Psychologist Frank Rosenblatt (1958) created the Perceptron, an algorithm for pattern recognition.

The 1st functional networks with many layers were published by Alexey Ivakhnenko and Lapa in 1965,
becoming the Group Method of Data Handling, a method of inductive statistical learning, for which
Ivakhnenko is sometimes referred as 'the father of deep learning'.

Neural network research stagnated after machine learning research by Minsky and Papert (1969) who
discovered two key issues:
1) Basic perceptrons were incapable of processing the exclusive-or circuit.
2) Computers didn't have enough processing power yet.

### Backpropagation





Concepts
neuroplasticity
Turing B-type machine
Frank Rosenblatt's perceptron
Group Method of Data Handling